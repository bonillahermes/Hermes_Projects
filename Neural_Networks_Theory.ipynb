{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN5h/ONJfpPYmAzBm4any/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonillahermes/Hermes_Projects/blob/main/Neural_Networks_Theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción a las redes neuronales"
      ],
      "metadata": {
        "id": "_-SWyVaKqQRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las redes neuronales son un pilar fundamental del aprendizaje profundo, una subdisciplina del aprendizaje automático, que a su vez es una rama de la inteligencia artificial (IA). Inspiradas por el funcionamiento del cerebro humano, estas redes son capaces de aprender tareas complejas a partir de grandes cantidades de datos. Han encontrado aplicaciones en diversos campos, como el reconocimiento de voz, la visión por computadora, y la traducción automática, entre otros."
      ],
      "metadata": {
        "id": "LEekL0D-q3JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué es una Red Neuronal?\n",
        "Una red neuronal artificial es un sistema de algoritmos que intenta reconocer patrones subyacentes en un conjunto de datos a través de un proceso que imita la forma en que el cerebro humano opera. En este sentido, las redes neuronales son capaces de interpretar datos sensoriales a través de una especie de simulación cerebral compuesta por capas de nodos, o \"neuronas\"."
      ],
      "metadata": {
        "id": "VgkQPMEkq4LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breve Historia de las Redes Neuronales\n",
        "\n",
        "La idea de las redes neuronales se remonta a los años 40, pero no fue hasta las últimas décadas que la tecnología y los algoritmos alcanzaron la madurez necesaria para su aplicación práctica. Los avances en el poder computacional, junto con la disponibilidad de grandes conjuntos de datos, han permitido el desarrollo de redes neuronales profundas, que son capaces de aprender tareas cada vez más complejas.\n",
        "\n",
        "- 1943 - Warren McCulloch y Walter Pitts publicaron el primer concepto de una neurona artificial.\n",
        "- 1960 - Frank Rosenblatt inventó el Perceptrón, el antecesor de las redes neuronales modernas.\n",
        "- 1980s - El resurgimiento del interés en las redes neuronales gracias al desarrollo del algoritmo de retropropagación.\n",
        "- Actualidad - Vivimos en la era del \"aprendizaje profundo\", donde las redes neuronales se aplican en una variedad de tecnologías avanzadas y campos de investigación."
      ],
      "metadata": {
        "id": "hIqbJUlJrAM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conceptos Fundamentales de las Redes Neuronales\n",
        "\n",
        "Para entender cómo funcionan las redes neuronales, es crucial familiarizarse con algunos conceptos fundamentales que forman la base de todas las redes neuronales, independientemente de su complejidad.\n",
        "\n",
        "## Neuronas Artificiales\n",
        "La neurona artificial, o perceptrón, es el bloque de construcción básico de una red neuronal. Inspirada en las neuronas biológicas del cerebro humano, una neurona artificial recibe entradas, las procesa, y genera una salida. Cada entrada tiene asociado un peso, que modifica la importancia de esta entrada. La suma ponderada de las entradas, ajustada por un término de sesgo (bias), se pasa a través de una función de activación para producir la salida de la neurona.\n",
        "\n",
        "## Funciones de Activación\n",
        "La función de activación es fundamental en la capacidad de la red neuronal para capturar relaciones no lineales. Sin ella, la red no sería diferente de un modelo lineal. Algunas de las funciones de activación más comunes incluyen:\n",
        "\n",
        "- ReLU (Rectified Linear Unit): Permite el paso de valores positivos sin cambiar, mientras que los valores negativos se convierten a cero. Es la más utilizada por su simplicidad computacional y eficacia.\n",
        "\n",
        "- Sigmoid: Transforma los valores de entrada en un rango entre 0 y 1, útil para modelos donde necesitamos probabilidades como salida.\n",
        "\n",
        "- Tanh (Tangente Hiperbólica): Similar a la sigmoid, pero transforma los valores en un rango entre -1 y 1, centrándolos alrededor de cero.\n",
        "\n",
        "## Arquitectura de una Red Neuronal\n",
        "Una red neuronal típica está compuesta por capas que contienen neuronas artificiales:\n",
        "\n",
        "1. Capa de Entrada: Recibe los datos de entrada para la red. El número de neuronas en esta capa debe coincidir con el número de características de los datos de entrada.\n",
        "\n",
        "2. Capas Ocultas: Capas intermedias que procesan las señales recibidas de la capa de entrada mediante pesos, biases, y funciones de activación. Pueden ser una o más.\n",
        "\n",
        "3. Capa de Salida: Produce el resultado final de la red. El número de neuronas en esta capa depende del tipo de tarea que la red está diseñada para realizar (por ejemplo, clasificación, regresión).\n",
        "\n",
        "## Ejemplo Conceptual de Pesos y Bias\n",
        "\n",
        "Imagina que estás tratando de predecir si el clima de mañana será lluvioso o soleado basado en dos entradas: la cantidad de nubes (%) y la humedad (%). Una neurona en tu red podría estar tratando de aprender esta relación para hacer la predicción.\n",
        "\n",
        "- **Entradas**:\n",
        "  - Cantidad de nubes (%): `40`\n",
        "  - Humedad (%): `70`\n",
        "\n",
        "Supongamos que inicialmente se asignan los siguientes **pesos** y **bias**:\n",
        "\n",
        "- **Pesos**:\n",
        "  - Peso asociado a la cantidad de nubes: `0.25`\n",
        "  - Peso asociado a la humedad: `0.5`\n",
        "- **Bias**: `-30`\n",
        "\n",
        "La operación que realiza la neurona se vería algo así:\n",
        "\n",
        "1. Calcula la suma ponderada de las entradas:\n",
        "   $$ (Cantidad\\ de\\ nubes \\times Peso\\ nubes) + (Humedad \\times Peso\\ humedad) + Bias $$\n",
        "   $$ (40 \\times 0.25) + (70 \\times 0.5) - 30 = 10 + 35 - 30 = 15 $$\n",
        "\n",
        "2. Este resultado se pasa a través de una función de activación (por ejemplo, una función sigmoide) para determinar la salida de la neurona. Suponiendo una función sigmoide, la salida sería calculada como:\n",
        "   $$ \\sigma(15) = \\frac{1}{1 + e^{-15}} \\approx 1 $$\n",
        "   Este resultado cercano a 1 podría interpretarse como una alta probabilidad de lluvia.\n",
        "\n",
        "### Interpretación\n",
        "\n",
        "En este ejemplo, el **peso** asociado a la humedad es más alto que el peso para la cantidad de nubes, lo que indica que la humedad tiene una mayor influencia en la predicción de lluvia según nuestra red. El **bias** de `-30` actúa como un umbral que ajusta el punto de partida de nuestra suma ponderada; en este caso, hace que sea necesario tener valores relativamente altos en las entradas para superar el umbral y predecir lluvia (resultado de la función de activación cercano a 1).\n",
        "\n",
        "### Importancia de Ajustar Pesos y Bias\n",
        "\n",
        "Durante el entrenamiento de una red neuronal, los pesos y biases se ajustan iterativamente a través de procesos como el descenso del gradiente y la retropropagación. Este ajuste fino permite que la red aprenda de manera efectiva a partir de los datos de entrenamiento, mejorando su capacidad para hacer predicciones precisas sobre datos no vistos.\n",
        "\n",
        "Este ejemplo simplifica muchos aspectos de cómo las redes neuronales operan en la práctica, pero captura la esencia de cómo los pesos y el bias influyen en el procesamiento de la información dentro de una neurona.\n"
      ],
      "metadata": {
        "id": "VNhWPT6ctCax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cómo Funcionan las Redes Neuronales\n",
        "Las redes neuronales aprenden y hacen predicciones a través de un proceso iterativo que implica dos fases principales: la propagación hacia adelante y la retropropagación. Este proceso permite que la red ajuste sus pesos y bias para mejorar sus predicciones.\n",
        "\n",
        "## Propagación Hacia Adelante\n",
        "La propagación hacia adelante es el proceso mediante el cual las entradas pasan a través de la red neuronal para producir una salida. En cada capa, las entradas son procesadas por las neuronas (usando los pesos, biases y funciones de activación) para generar salidas, que luego se convierten en las entradas para la siguiente capa.\n",
        "\n",
        "**Ejemplo de Propagación Hacia Adelante:**\n",
        "\n",
        "Supongamos que tenemos una red neuronal simple con una capa oculta. La entrada \\(X\\) pasa a través de la capa oculta donde se transforma utilizando pesos \\(W\\) y bias \\(b\\), y luego se aplica una función de activación \\(f\\). La salida de esta capa se utiliza como entrada para la capa de salida, que produce la predicción final \\(Y\\).\n",
        "\n",
        "## Función de Pérdida\n",
        "La función de pérdida mide cuán bien la red neuronal está realizando sus predicciones en comparación con los valores reales. El objetivo del entrenamiento es minimizar esta función de pérdida, lo que indica que la red está aprendiendo correctamente de los datos de entrenamiento.\n",
        "\n",
        "**Ejemplos Comunes de Funciones de Pérdida:**\n",
        "\n",
        "- **Error Cuadrático Medio (MSE)**: Usado principalmente en problemas de regresión.\n",
        "- **Entropía Cruzada Binaria**: Usada en problemas de clasificación binaria.\n",
        "- **Entropía Cruzada Categórica**: Usada en problemas de clasificación multiclase.\n",
        "\n",
        "## Retropropagación y Descenso del Gradiente: Una Mirada Técnica\n",
        "\n",
        "La **retropropagación** y el **descenso del gradiente** son dos conceptos fundamentales en el entrenamiento de redes neuronales. Juntos, permiten que una red ajuste sus parámetros internos (pesos y biases) para minimizar la función de pérdida, mejorando así la precisión de sus predicciones.\n",
        "\n",
        "### Retropropagación\n",
        "\n",
        "La retropropagación es un método para calcular el gradiente de la función de pérdida con respecto a cada peso en la red, lo cual es esencial para el proceso de optimización. Se basa en la regla de la cadena del cálculo diferencial para retropropagar el error desde la salida hacia las capas anteriores de la red.\n",
        "\n",
        "**Pasos Clave:**\n",
        "\n",
        "1. **Propagación hacia adelante**: Calcula las salidas de la red (y la función de pérdida) dados los valores actuales de los pesos y biases.\n",
        "2. **Cálculo del gradiente de la función de pérdida**: Evalúa cuánto cambia la función de pérdida con respecto a cada peso y bias en la red, comenzando desde la última capa y moviéndose hacia atrás.\n",
        "3. **Actualización de pesos y biases**: Utiliza estos gradientes para actualizar los pesos y biases, típicamente moviéndolos en la dirección opuesta al gradiente para minimizar la función de pérdida.\n",
        "\n",
        "### Descenso del Gradiente\n",
        "\n",
        "El descenso del gradiente es un algoritmo de optimización utilizado para minimizar la función de pérdida ajustando iterativamente los parámetros de la red. Se calcula el gradiente de la función de pérdida con respecto a cada parámetro, y luego se ajustan los parámetros en la dirección opuesta al gradiente.\n",
        "\n",
        "**Fórmula de Actualización de Parámetros:**\n",
        "\n",
        "$$\n",
        "\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta)\n",
        "$$\n",
        "\n",
        "Donde:\n",
        "-  $\\theta$  representa un parámetro de la red (peso o bias).\n",
        "- $ \\eta $ es la tasa de aprendizaje, que controla el tamaño del paso que se toma en la dirección opuesta al gradiente.\n",
        "- $ \\nabla_{\\theta} J(\\theta) $ es el gradiente de la función de pérdida \\( J \\) con respecto al parámetro $ \\theta $.\n",
        "\n",
        "### Importancia del Aprendizaje\n",
        "\n",
        "A través de múltiples iteraciones de propagación hacia adelante y retropropagación, ajustando los parámetros con el método de descenso del gradiente, la red neuronal aprende gradualmente optimizando sus pesos y biases para minimizar la función de pérdida. Este proceso iterativo se repite hasta que la red alcanza un nivel de precisión satisfactorio o hasta que se cumpla un criterio de parada específico.\n",
        "\n",
        "La combinación de retropropagación y descenso del gradiente hace posible entrenar redes neuronales profundas, permitiéndoles aprender de conjuntos de datos complejos y realizar tareas que antes se consideraban inalcanzables para las máquinas.\n"
      ],
      "metadata": {
        "id": "9G0RisKuvwbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construyendo una Red Neuronal Simple con Python\n",
        "En esta sección, aprenderemos cómo construir y entrenar una red neuronal básica con Python utilizando la biblioteca Keras. Para este ejemplo, utilizaremos el conjunto de datos MNIST, que consta de imágenes de dígitos manuscritos del 0 al 9. Nuestro objetivo será entrenar un modelo para reconocer estos dígitos.\n",
        "\n",
        "## Preparación del Entorno\n",
        "Primero, asegúrate de tener instalado TensorFlow y Keras. En Colab, estos deberían estar preinstalados, pero puedes asegurarte ejecutando:"
      ],
      "metadata": {
        "id": "kIjkk5BSzQz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzxL7Ti9p-at"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de Librerías"
      ],
      "metadata": {
        "id": "QUNIPXMTzhL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "Wu5BcjEozdTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargando el Conjunto de Datos MNIST\n",
        "Keras facilita la carga de varios conjuntos de datos comunes a través de `keras.datasets`. Carguemos el conjunto de datos MNIST y preparemos nuestros datos de entrenamiento y prueba:"
      ],
      "metadata": {
        "id": "ZpLTpzpezsWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos MNIST\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalizar los datos de entrada\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
      ],
      "metadata": {
        "id": "KZsEqHipznwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_test_one_hot = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "Wknrctbg78P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construcción del Modelo\n",
        "Construiremos una red neuronal simple con una capa de entrada que aplana la imagen de 28x28 en un vector de 784 elementos, una capa oculta con 128 neuronas y función de activación ReLU, y una capa de salida con 10 neuronas correspondientes a las 10 clases de dígitos, usando la función de activación softmax."
      ],
      "metadata": {
        "id": "xw5zvdMnz64B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "  Flatten(input_shape=(28, 28)),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "HFCHAXEwz86d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compilación del Modelo\n",
        "Antes de entrenar el modelo, necesitamos especificar el optimizador, la función de pérdida y las métricas para evaluar:"
      ],
      "metadata": {
        "id": "HAWne0QE0NTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "              run_eagerly=True)"
      ],
      "metadata": {
        "id": "A06bUbHy0UxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del Modelo\n",
        "Ahora, entrenamos el modelo con nuestros datos de entrenamiento:"
      ],
      "metadata": {
        "id": "PkxEc_9f0ZBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train_one_hot, epochs=5, validation_split=0.2)"
      ],
      "metadata": {
        "id": "JBDuxIf-0bRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación y Predicción\n",
        "Finalmente, evaluamos el rendimiento del modelo en el conjunto de datos de prueba y hacemos algunas predicciones:"
      ],
      "metadata": {
        "id": "PQC1yfE60hqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo\n",
        "results = model.evaluate(x_test, y_test_one_hot)\n",
        "test_loss, test_acc, test_precision, test_recall = results\n",
        "print('\\nTest loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Test precision:', test_precision)\n",
        "print('Test recall:', test_recall)\n"
      ],
      "metadata": {
        "id": "Inebdmx-0fGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las métricas de entrenamiento del historial\n",
        "train_loss = history.history['loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "train_precision = history.history['precision_7']\n",
        "train_recall = history.history['recall_7']\n",
        "\n",
        "# Crear un gráfico de líneas para cada métrica de entrenamiento\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.plot(train_accuracy, label='Train Accuracy')\n",
        "plt.plot(train_precision, label='Train Precision')\n",
        "plt.plot(train_recall, label='Train Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Training Metrics')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Tz3b31u3u0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}